{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5d03f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Annotated, Optional, List, Dict, Any\n",
    "from langchain_core.messages import ToolMessage, AIMessage, SystemMessage, HumanMessage\n",
    "from langchain_core.tools import InjectedToolCallId, tool\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.types import Command, interrupt\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable \n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.types import Interrupt,Command\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import yaml\n",
    "import re\n",
    "import textwrap\n",
    "import json\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8919a532",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1 ¬∑ InputState ‚Äì what arrives on the very first invoke()\n",
    "# ---------------------------------------------------------------------\n",
    "class InputState(BaseModel):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2 ¬∑ IntermediateState ‚Äì everything nodes may read/write\n",
    "# ---------------------------------------------------------------------\n",
    "class IntermediateState(BaseModel):\n",
    "    # flags & classification\n",
    "    on_topic: Optional[bool] = None\n",
    "\n",
    "    # employee attributes\n",
    "    on_project: Optional[int] = None\n",
    "    tech_stack: Optional[List[str]] = None\n",
    "    exp: Optional[int] = None\n",
    "    degree_level: Optional[str] = None\n",
    "\n",
    "    # HITL bookkeeping\n",
    "    missing_fields: List[str] = Field(default_factory=list)\n",
    "    all_fields_present: bool = False\n",
    "    hitl_pending: Optional[str] = None \n",
    "\n",
    "    # SQL generation inputs\n",
    "    db_name: Optional[str] = \"employees\"\n",
    "    table: Optional[str] = \"employees\"\n",
    "    fields: List[str] = Field(default_factory=list)\n",
    "    limit: Optional[int] = None\n",
    "\n",
    "    # SQL outputs\n",
    "    sql_query: Optional[str] = None\n",
    "    query_params: List[Any] = Field(default_factory=list)\n",
    "    query_results: List[Dict[str, Any]] = Field(default_factory=list)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3 ¬∑ OutputState ‚Äì what you want back from graph.invoke()\n",
    "# ---------------------------------------------------------------------\n",
    "class OutputState(BaseModel):\n",
    "    results: List[Dict[str, Any]] = Field(default_factory=list)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 4 ¬∑ OverallState ‚Äì single internal schema seen by every node\n",
    "# ---------------------------------------------------------------------\n",
    "class OverallState(InputState, IntermediateState, OutputState):\n",
    "    \"\"\"Merged state; nodes accept one `state: OverallState` parameter.\"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04c09edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a highly skilled Resource Allocation Agent for a tech company. Your task is to find and suggest the most suitable employees for internal project needs.\n",
    "You analyze employee attributes such as *exp* (in years), *on_project* status (0 = available, 1 = occupied), *tech_stack*, and *degree_level* (Bachelor, Master, PhD).\n",
    "Respond clearly, ask follow‚Äëup questions when needed, and always aim to provide actionable results based on user input.\n",
    "‚ö†Ô∏è Use only the exact database column names: exp, on_project, tech_stack, degree_level.\n",
    "Do NOT use synonyms like \"experience\" instead of \"exp\", or \"skills\" instead of \"tech_stack\".\n",
    "If the question is unrelated to employee resource allocation, reply: \"Sorry, I can only help with employee resource allocation questions.\"\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "def _latest_user(state) -> Optional[str]:\n",
    "    # Helper to fetch the most recent user input message\n",
    "    for msg in reversed(state.messages):\n",
    "        if isinstance(msg, HumanMessage):\n",
    "            return msg.content.strip()\n",
    "    return None\n",
    "\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\")  # LLM model used for all chains\n",
    "\n",
    "\n",
    "\n",
    "EMPLOYEE_FIELDS = [\"exp\", \"tech_stack\", \"on_project\", \"degree_level\"]         # already validated helper\n",
    "\n",
    "field_extraction_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are an intelligent assistant that extracts structured field-value \"\n",
    "        \"pairs from a user's natural language query. \"\n",
    "        f\"The employee table contains the following fields: {', '.join(EMPLOYEE_FIELDS)}. \"\n",
    "        \"If the user provides any of these explicitly or implicitly, extract them into a JSON object. \"\n",
    "        \"For example, if the user says 'available', map that to `on_project=0`. \"\n",
    "        \"Only include keys that are clearly present in the query. For `tech_stack`, return a list.\"\n",
    "    ),\n",
    "    (\"human\", \"{query}\")\n",
    "])\n",
    "\n",
    "extractor_chain = field_extraction_prompt | llm | (lambda x: x.content.strip())\n",
    "\n",
    "\n",
    "def _clean_llm_output(raw: str) -> str:\n",
    "    \"\"\"Remove ```json fences and return the first {...} block.\"\"\"\n",
    "    cleaned = textwrap.dedent(raw).strip(\"` \\n\")\n",
    "    match = re.search(r\"\\{[\\s\\S]*?\\}\", cleaned)\n",
    "    return match.group(0) if match else \"{}\"\n",
    "\n",
    "\n",
    "def field_extractor_node(state: OverallState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Uses an LLM to extract employee attributes (e.g., exp, tech_stack) from the user's input.\n",
    "    Also identifies which fields are still missing.\n",
    "    \"\"\"\n",
    "    user_msg = _latest_user(state)\n",
    "    if not user_msg:\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        raw_llm = extractor_chain.invoke({\"query\": user_msg})\n",
    "        print(\"[field_extractor] LLM raw:\", raw_llm)\n",
    "        json_text = _clean_llm_output(raw_llm)\n",
    "        extracted = json.loads(json_text)\n",
    "    except Exception as err:\n",
    "        print(f\"[field_extractor] ‚ö†Ô∏è Cannot parse LLM output: {err}\")\n",
    "        extracted = {}\n",
    "\n",
    "    # Keep only valid employee fields\n",
    "    updates: Dict[str, Any] = {k: v for k, v in extracted.items() if k in EMPLOYEE_FIELDS}\n",
    "\n",
    "    # Determine what still needs to be collected from the user\n",
    "    missing = [f for f in EMPLOYEE_FIELDS if f not in updates]\n",
    "    updates.update(\n",
    "        missing_fields=missing,\n",
    "        all_fields_present=len(missing) == 0,\n",
    "    )\n",
    "    return updates\n",
    "\n",
    "\n",
    "def hitl_node(state: OverallState) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Human-in-the-loop node: asks the user for any missing fields (like exp or degree_level).\n",
    "    Adds the response to the state.\n",
    "    \"\"\"\n",
    "    if not state.missing_fields:\n",
    "        return {}\n",
    "\n",
    "    field = state.missing_fields[0]\n",
    "    question = f\"Please provide a value for **{field}**:\"\n",
    "    answer = interrupt(question)\n",
    "\n",
    "    # Normalize types based on field type\n",
    "    if field in {\"on_project\", \"exp\"}:\n",
    "        try:\n",
    "            answer = int(answer)\n",
    "        except Exception:\n",
    "            pass\n",
    "    elif field == \"tech_stack\":\n",
    "        answer = [tok.strip() for tok in str(answer).split(\",\") if tok.strip()]\n",
    "\n",
    "    remaining = state.missing_fields[1:]\n",
    "\n",
    "    # Return only the updates (LangGraph merges this into full state)\n",
    "    return {\n",
    "        field: answer,\n",
    "        \"missing_fields\": remaining,\n",
    "        \"all_fields_present\": len(remaining) == 0,\n",
    "        \"messages\": [\n",
    "            AIMessage(content=question),\n",
    "            HumanMessage(content=str(answer)),\n",
    "        ],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cd75d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"generate_sql_query_and_fetch_db_results\")\n",
    "def generate_and_run_employee_query(\n",
    "    *,\n",
    "    db_name: str,\n",
    "    exp: int = None,\n",
    "    on_project: int = None,\n",
    "    tech_stack: list = None,\n",
    "    degree_level: str = None,\n",
    "    limit: int = 10\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Generates and executes a SQL query on the `employees` table to find suitable employees for internal project allocation.\n",
    "\n",
    "    Use this tool when the user is asking to find employees based on filters like:\n",
    "    - Minimum years of experience (`exp`)\n",
    "    - Whether the employee is currently available or occupied (`on_project`)\n",
    "    - Required technical skills (`tech_stack`)\n",
    "    - Educational qualification (`degree_level`)\n",
    "\n",
    "    This tool returns a list of matching employees directly from the database.\n",
    "    \"\"\"\n",
    "    from psycopg2.extensions import adapt\n",
    "    import psycopg2\n",
    "    import json\n",
    "    from database.config_db import discover_databases\n",
    "    from langchain.prompts import ChatPromptTemplate\n",
    "    from langchain_groq import ChatGroq\n",
    "\n",
    "    TOOL_PROMPT = \"\"\"\n",
    "You are an expert PostgreSQL query generator. Based on the inputs below, return a parameterized SELECT query using %s placeholders.\n",
    "Use table_name as *employees*\n",
    "- exp ‚Üí use exp >= %s\n",
    "- on_project ‚Üí on_project = %s\n",
    "- tech_stack ‚Üí tech_stack @> %s\n",
    "- degree_level ‚Üí degree_level = %s\n",
    "Always include a LIMIT %s clause.\n",
    "Only include filters for non-null fields.\n",
    "\n",
    "Inputs:\n",
    "- exp: {exp}\n",
    "- on_project: {on_project}\n",
    "- tech_stack: {tech_stack}\n",
    "- degree_level: {degree_level}\n",
    "- limit: {limit}\n",
    "\n",
    "Respond ONLY with a JSON object: {{\"query\": \"...\", \"params\": [...]}} ‚Äî no markdown, no explanations.\n",
    "\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(TOOL_PROMPT)\n",
    "    llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0.0)\n",
    "    chain = prompt | llm\n",
    "\n",
    "    response = chain.invoke({\n",
    "    \"exp\": exp,\n",
    "    \"on_project\": on_project,\n",
    "    \"tech_stack\": tech_stack,\n",
    "    \"degree_level\": degree_level,\n",
    "    \"limit\": limit\n",
    "})\n",
    "\n",
    "    response_text = response.content.strip().strip(\"`\")\n",
    "    if response_text.startswith(\"json\"):\n",
    "        response_text = response_text[len(\"json\"):].strip()\n",
    "\n",
    "    print(\"[üß† LLM Raw Response]\", repr(response_text))\n",
    "\n",
    "    try:\n",
    "        parsed = json.loads(response_text)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"‚ùå Failed to parse LLM response: {e}\\nRaw: {response_text}\")\n",
    "\n",
    "    query = parsed[\"query\"]\n",
    "    params = parsed[\"params\"]\n",
    "    try:\n",
    "        db = discover_databases()[\"employees\"]\n",
    "        conn = psycopg2.connect(\n",
    "            host=db[\"host\"], port=db[\"port\"], dbname=db[\"dbname\"],\n",
    "            user=db[\"owner\"], password=db[\"password\"]\n",
    "        )\n",
    "\n",
    "        # Ensure list-type (tech_stack) params are kept as native Python lists\n",
    "        clean_params = []\n",
    "        for p in params:\n",
    "            if isinstance(p, str) and p.startswith(\"{\") and p.endswith(\"}\"):\n",
    "                p = p.strip(\"{}\").split(\",\")\n",
    "                p = [x.strip(\" \\\"'\") for x in p]\n",
    "            clean_params.append(p)\n",
    "\n",
    "        print(\"[Running Query]\")\n",
    "        print(\"SQL Query:\", query)\n",
    "        print(\"Params:\", clean_params)\n",
    "\n",
    "        with conn, conn.cursor() as cur:\n",
    "            cur.execute(query, clean_params)\n",
    "            cols = [desc.name for desc in cur.description]\n",
    "            rows = [dict(zip(cols, row)) for row in cur.fetchall()]\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f\"[resource_allocator] ‚ö† {err} ‚Äî using fallback\")\n",
    "        rows = [\n",
    "            {\"id\": 999, \"first_name\": \"Stub\", \"last_name\": \"User\", \"on_project\": 0,\n",
    "             \"exp\": 42, \"tech_stack\": [\"python\", \"sql\"], \"degree_level\": \"PhD\"}\n",
    "        ]\n",
    "\n",
    "    return json.dumps({\"results\": rows})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d82b89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_node(state: OverallState) -> Dict[str, Any]:\n",
    "    print(\"[tool_node] Calling resource_allocator tool via LLM\")\n",
    "    state.messages.append(HumanMessage(content=_latest_user(state)))\n",
    "\n",
    "    response = llm.bind_tools([generate_and_run_employee_query]).invoke(state.messages)\n",
    "    if not response.tool_calls:\n",
    "        raise ValueError(\"[tool_node] ‚ùå No tool call found\")\n",
    "\n",
    "    tool_call = response.tool_calls[0]\n",
    "    tool_output = generate_and_run_employee_query.invoke({\n",
    "        \"db_name\": state.db_name,\n",
    "        \"exp\": state.exp,\n",
    "        \"on_project\": state.on_project,\n",
    "        \"tech_stack\": state.tech_stack,\n",
    "        \"degree_level\": state.degree_level,\n",
    "        \"limit\": state.limit or 10\n",
    "    }, tool_id=tool_call[\"id\"])\n",
    "\n",
    "    state.messages.append(ToolMessage(content=tool_output, tool_call_id=tool_call[\"id\"]))\n",
    "    parsed = json.loads(tool_output)\n",
    "    rows = parsed[\"results\"]\n",
    "\n",
    "    # Let LLM summarize the results directly\n",
    "    summary_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", SYSTEM_PROMPT),\n",
    "        (\"human\", f\"Given the following result rows, summarize suitable employees:\\n{rows}\")\n",
    "    ])\n",
    "    print(\"Inside tool_node\",SYSTEM_PROMPT )\n",
    "    summary_chain = summary_prompt | llm\n",
    "    summary = summary_chain.invoke({}).content.strip()\n",
    "\n",
    "    return {\"messages\": [AIMessage(content=summary)], \"results\": rows}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "628eacd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_after_extractor(state: OverallState) -> str:\n",
    "    \"\"\"\n",
    "    Router called after `field_extractor_node`.\n",
    "\n",
    "    - If all fields are present ‚Üí proceed to the `tool_node`\n",
    "    - Else ‚Üí ask user missing fields via the `hitl` node\n",
    "    \"\"\"\n",
    "    return \"tool_node\" if state.all_fields_present else \"hitl\"\n",
    "\n",
    "def route_after_hitl(state: OverallState) -> str:\n",
    "    \"\"\"\n",
    "    Router called after resuming from HITL interaction.\n",
    "\n",
    "    - If all fields are now present ‚Üí proceed to the `tool_node`\n",
    "    - Else ‚Üí stay in the HITL loop\n",
    "    \"\"\"\n",
    "    return \"tool_node\" if state.all_fields_present else \"hitl\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ede0f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "load_dotenv()\n",
    "\n",
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\",api_key=os.getenv('GROQ_API_KEY'))\n",
    "llm = llm.bind_tools([generate_and_run_employee_query])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "154c28a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1 ¬∑ Initialise the builder with overall / I‚ÄëO schemas\n",
    "# ---------------------------------------------------------------------\n",
    "builder = StateGraph(\n",
    "    OverallState,           # internal state\n",
    "    input=InputState,\n",
    "    output=OverallState     # ‚Üê change here\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2 ¬∑ Register all nodes (single‚Äëarg versions)\n",
    "# ---------------------------------------------------------------------\n",
    "builder.add_node(\"field_extractor\",  field_extractor_node)\n",
    "builder.add_node(\"hitl\",             hitl_node)\n",
    "builder.add_node(\"tool_node\",      tool_node)\n",
    "builder.set_entry_point(\"field_extractor\")\n",
    "\n",
    "# And update routing accordingly\n",
    "builder.add_conditional_edges(\n",
    "    \"field_extractor\",\n",
    "    route_after_extractor,\n",
    "    path_map=[\"tool_node\", \"hitl\"]   # ‚Üê updated target\n",
    ")\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"hitl\",\n",
    "    route_after_hitl,\n",
    "    path_map=[\"tool_node\", \"hitl\"]   # ‚Üê updated target\n",
    ")\n",
    "\n",
    "builder.add_edge(\"tool_node\", END)\n",
    "# ---------------------------------------------------------------------\n",
    "# 4 ¬∑ Compile with an in‚Äëmemory checkpointer\n",
    "# ---------------------------------------------------------------------\n",
    "graph = builder.compile(checkpointer=MemorySaver())        # :contentReference[oaicite:3]{index=3}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "035564a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6432720a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[field_extractor] LLM raw: Here's the extracted JSON object:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"exp\": 4,\n",
      "  \"tech_stack\": [\"Python\", \"FastAPI\", \"AWS\"]\n",
      "}\n",
      "```\n",
      "[tool_node] Calling resource_allocator tool via LLM\n",
      "[üß† LLM Raw Response] '{\"query\": \"SELECT * FROM employees WHERE exp >= %s AND on_project = %s AND tech_stack @> %s AND degree_level = %s LIMIT %s\", \"params\": [4, 1, \"{\\'Python\\', \\'FastAPI\\', \\'AWS\\'}\", \"Bachelor\", 10]}'\n",
      "[Running Query]\n",
      "SQL Query: SELECT * FROM employees WHERE exp >= %s AND on_project = %s AND tech_stack @> %s AND degree_level = %s LIMIT %s\n",
      "Params: [4, 1, ['Python', 'FastAPI', 'AWS'], 'Bachelor', 10]\n",
      "Inside tool_node You are a highly skilled Resource Allocation Agent for a tech company. Your task is to find and suggest the most suitable employees for internal project needs.\n",
      "You analyze employee attributes such as *exp* (in years), *on_project* status (0 = available, 1 = occupied), *tech_stack*, and *degree_level* (Bachelor, Master, PhD).\n",
      "Respond clearly, ask follow‚Äëup questions when needed, and always aim to provide actionable results based on user input.\n",
      "‚ö†Ô∏è Use only the exact database column names: exp, on_project, tech_stack, degree_level.\n",
      "Do NOT use synonyms like \"experience\" instead of \"exp\", or \"skills\" instead of \"tech_stack\".\n",
      "If the question is unrelated to employee resource allocation, reply: \"Sorry, I can only help with employee resource allocation questions.\"\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Input to ChatPromptTemplate is missing variables {\"\\'id\\'\"}.  Expected: [\"\\'id\\'\"] Received: []\\nNote: if you intended {\\'id\\'} to be part of the string and not a variable, please escape it with double curly braces like: \\'{{\\'id\\'}}\\'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT '",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m     user_answer = builtins.input(prompt)\n\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m# resume the graph with that answer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     result = \u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCommand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_answer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconfigurable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthread_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdemo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# ---- dump the final chat -----------------------------------------------------\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m msg \u001b[38;5;129;01min\u001b[39;00m result[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ram\\Ascentt Projects\\resource_planner_agent\\venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2719\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2716\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]] = []\n\u001b[32m   2717\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2719\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2720\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2721\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2722\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2723\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2724\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2725\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2726\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2727\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2728\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2729\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2730\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2731\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2732\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2733\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mints\u001b[49m\u001b[43m \u001b[49m\u001b[43m:=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINTERRUPT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m   2734\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ram\\Ascentt Projects\\resource_planner_agent\\venv\\Lib\\site-packages\\langgraph\\pregel\\__init__.py:2436\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2434\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2435\u001b[39m             loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2436\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2437\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2438\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2439\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2440\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2441\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2442\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2443\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2444\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ram\\Ascentt Projects\\resource_planner_agent\\venv\\Lib\\site-packages\\langgraph\\pregel\\runner.py:161\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    159\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ram\\Ascentt Projects\\resource_planner_agent\\venv\\Lib\\site-packages\\langgraph\\pregel\\retry.py:40\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     38\u001b[39m     task.writes.clear()\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     42\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ram\\Ascentt Projects\\resource_planner_agent\\venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py:623\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    621\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m623\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    625\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ram\\Ascentt Projects\\resource_planner_agent\\venv\\Lib\\site-packages\\langgraph\\utils\\runnable.py:377\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    375\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mtool_node\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInside tool_node\u001b[39m\u001b[33m\"\u001b[39m,SYSTEM_PROMPT )\n\u001b[32m     29\u001b[39m summary_chain = summary_prompt | llm\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m summary = \u001b[43msummary_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m.content.strip()\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [AIMessage(content=summary)], \u001b[33m\"\u001b[39m\u001b[33mresults\u001b[39m\u001b[33m\"\u001b[39m: rows}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ram\\Ascentt Projects\\resource_planner_agent\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3045\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3043\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3044\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3045\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3046\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3047\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ram\\Ascentt Projects\\resource_planner_agent\\venv\\Lib\\site-packages\\langchain_core\\prompts\\base.py:216\u001b[39m, in \u001b[36mBasePromptTemplate.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.tags:\n\u001b[32m    215\u001b[39m     config[\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m] = config[\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m] + \u001b[38;5;28mself\u001b[39m.tags\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_format_prompt_with_error_handling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[43mserialized\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_serialized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ram\\Ascentt Projects\\resource_planner_agent\\venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1940\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1936\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1937\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1938\u001b[39m         output = cast(\n\u001b[32m   1939\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1942\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1944\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1945\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1946\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1947\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1948\u001b[39m         )\n\u001b[32m   1949\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1950\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ram\\Ascentt Projects\\resource_planner_agent\\venv\\Lib\\site-packages\\langchain_core\\runnables\\config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ram\\Ascentt Projects\\resource_planner_agent\\venv\\Lib\\site-packages\\langchain_core\\prompts\\base.py:189\u001b[39m, in \u001b[36mBasePromptTemplate._format_prompt_with_error_handling\u001b[39m\u001b[34m(self, inner_input)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: \u001b[38;5;28mdict\u001b[39m) -> PromptValue:\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     _inner_input = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_prompt(**_inner_input)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ram\\Ascentt Projects\\resource_planner_agent\\venv\\Lib\\site-packages\\langchain_core\\prompts\\base.py:183\u001b[39m, in \u001b[36mBasePromptTemplate._validate_input\u001b[39m\u001b[34m(self, inner_input)\u001b[39m\n\u001b[32m    177\u001b[39m     example_key = missing.pop()\n\u001b[32m    178\u001b[39m     msg += (\n\u001b[32m    179\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mNote: if you intended \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m to be part of the string\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    180\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m and not a variable, please escape it with double curly braces like: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    181\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mexample_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    182\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[32m    184\u001b[39m         create_message(message=msg, error_code=ErrorCode.INVALID_PROMPT_INPUT)\n\u001b[32m    185\u001b[39m     )\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m inner_input\n",
      "\u001b[31mKeyError\u001b[39m: 'Input to ChatPromptTemplate is missing variables {\"\\'id\\'\"}.  Expected: [\"\\'id\\'\"] Received: []\\nNote: if you intended {\\'id\\'} to be part of the string and not a variable, please escape it with double curly braces like: \\'{{\\'id\\'}}\\'.\\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/INVALID_PROMPT_INPUT '",
      "During task with name 'tool_node' and id '2d7c58b9-9c9f-a951-34ab-9aaf1d640e7f'"
     ]
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# Driver / test loop ‚Äì paste this into a fresh cell\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "import builtins                                    # gives us builtins.input\n",
    "from langgraph.types import Command                # already used for resume\n",
    "from langchain_core.messages import HumanMessage   # convenience\n",
    "# Hi I need an AI engineer -> store the list of parameters for AI Engineer and \n",
    "# ---- kick off the first turn -------------------------------------------------\n",
    "state = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"Find an employee who is good in Python, FastAPI, AWS with 4 year of exp\")\n",
    "    ]\n",
    "}\n",
    "#Find an employee with 5 yrs exp and good hold in ML\n",
    "#Find an employee who is good in Python, FastAPI, AWS with 4 year of exp\n",
    "result = graph.invoke(state, config={\"configurable\": {\"thread_id\": \"demo\"}})\n",
    "\n",
    "# ---- handle any HITL pauses --------------------------------------------------\n",
    "while \"__interrupt__\" in result:\n",
    "    # grab the prompt the HITL node produced\n",
    "    prompt = result[\"__interrupt__\"][0].value + \" \"\n",
    "    # ask the real user for the missing value\n",
    "    user_answer = builtins.input(prompt)\n",
    "    # resume the graph with that answer\n",
    "    result = graph.invoke(\n",
    "        Command(resume=user_answer),\n",
    "        config={\"configurable\": {\"thread_id\": \"demo\"}}\n",
    "    )\n",
    "\n",
    "# ---- dump the final chat -----------------------------------------------------\n",
    "for msg in result[\"messages\"]:\n",
    "    print(type(msg).__name__, \"‚Üí\", msg.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e55460c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Find an employee who is good in Python, FastAPI, AWS with 4 year of exp', additional_kwargs={}, response_metadata={}, id='39e06781-31e4-4a4f-9ed8-09a2c2a364c5'),\n",
       " SystemMessage(content='You are a highly skilled Resource Allocation Agent for a tech company. Your task is to find and suggest the most suitable employees for internal project needs.\\nYou analyze employee attributes such as experience (in years), on_project status (0 = available, 1 = occupied), tech_stack, and degree_level (Bachelor, Master, PhD).\\nRespond clearly, ask follow‚Äëup questions when needed, and always aim to provide actionable results based on user input.\\n‚ö†Ô∏è Use only the exact database column names: exp, on_project, tech_stack, degree_level.\\nDo NOT use synonyms like \"experience\" instead of \"exp\", or \"skills\" instead of \"tech_stack\".', additional_kwargs={}, response_metadata={}, id='a39381bb-8c78-4fbe-afe0-60c8a5c09e52'),\n",
       " AIMessage(content=\"Hi, I'm your Resource Allocation Agent. How may I assist you today?\", additional_kwargs={}, response_metadata={}, id='393f1bfd-b8ac-4ae4-987e-fb987f0c641b'),\n",
       " AIMessage(content='Please provide a value for **on_project**:', additional_kwargs={}, response_metadata={}, id='79433608-e910-4947-8f23-54277624f005'),\n",
       " HumanMessage(content='1', additional_kwargs={}, response_metadata={}, id='a8fa6ca4-af47-4c71-b6f2-b5c9ee6f4e77'),\n",
       " AIMessage(content='Please provide a value for **degree_level**:', additional_kwargs={}, response_metadata={}, id='b166ffd5-b778-4825-8b9a-cf4542fc7a59'),\n",
       " HumanMessage(content='Bachelor', additional_kwargs={}, response_metadata={}, id='cf7cca5f-cc3b-46be-b2e3-e07fbb7d54bc'),\n",
       " HumanMessage(content='Bachelor', additional_kwargs={}, response_metadata={}, id='6124b14c-3eb2-475d-aad2-8412605d5c14'),\n",
       " ToolMessage(content='{\\n  \"query\": \"SELECT * FROM employees WHERE exp >= %s AND on_project = %s AND tech_stack @> %s AND degree_level = %s LIMIT %s\",\\n  \"params\": [4, 1, [\"Python\", \"FastAPI\", \"AWS\"], \"Bachelor\", 10]\\n}', id='2776b7f7-727b-481e-9744-07be3703611c', tool_call_id='kvjre2yc3'),\n",
       " AIMessage(content='Here are the best‚Äëmatched employees:\\n‚Ä¢ Alice Smith (4 yrs, stack: Python, FastAPI, AWS)', additional_kwargs={}, response_metadata={}, id='563e144b-62e3-4fda-9600-4a2eded14057')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c151f28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
